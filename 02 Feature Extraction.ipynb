{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Common Ternms**\n",
    "\n",
    "\n",
    "# üìò 1. Corpus\n",
    "**üîπ Easy Meaning:**\n",
    "A corpus is a big collection of texts (many documents or messages).\n",
    "Think of it like a folder full of files you want to study or train a model on.\n",
    "\n",
    "**üß† Example:**\n",
    "\n",
    "A folder with 1,000 emails = Corpus\n",
    "\n",
    "A collection of tweets = Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù 2. Vocabulary\n",
    "**üîπ Easy Meaning:**\n",
    "Vocabulary is the list of unique words found in your corpus.\n",
    "It shows what words are used, not how often.\n",
    "\n",
    "**üß† Example:**\n",
    "If your texts have:\n",
    "\n",
    "‚ÄúI love NLP‚Äù, ‚ÄúNLP is fun‚Äù, ‚ÄúI love Python‚Äù\n",
    "\n",
    "Vocabulary = {I, love, NLP, is, fun, Python}\n",
    "\n",
    "‚úÖ No repeats ‚Äî just the full word list.\n",
    "\n",
    "\n",
    "\n",
    "# üìÑ 3. Document\n",
    "**üîπ Easy Meaning:**\n",
    "A document is one piece of text inside your corpus.\n",
    "\n",
    "**üß† Example:**\n",
    "If the corpus is 1,000 emails,\n",
    "then each email = one document.\n",
    "\n",
    "**Another example:**\n",
    "\n",
    "One tweet\n",
    "\n",
    "One review\n",
    "\n",
    "One paragraph\n",
    "All are ‚Äúdocuments‚Äù.\n",
    "\n",
    "# üî§ 4. Word\n",
    "**üîπ Easy Meaning:**\n",
    "Just a single word in a document.\n",
    "\n",
    "**üß† Example:**\n",
    "In the sentence ‚ÄúI love Python‚Äù,\n",
    "the words are: I, love, Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Important NLP Feature Extraction Techniques**\n",
    "\n",
    "## **1. One-Hot Encoding (OHE)**  \n",
    "**üîπ Easy Meaning:**  \n",
    "Turn each word into a vector (a row of 0s and 1s) where only one word is 1, and all others are 0.  \n",
    "\n",
    "**üß† Example:**  \n",
    "Let‚Äôs say vocabulary = [\"cat\", \"dog\", \"fish\"]  \n",
    "\n",
    "- \"cat\" ‚Üí [1, 0, 0]  \n",
    "- \"dog\" ‚Üí [0, 1, 0]  \n",
    "- \"fish\" ‚Üí [0, 0, 1]  \n",
    "\n",
    "‚úÖ Simple but inefficient for large vocabularies.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üß∫ 2. BoW (Bag of Words)**  \n",
    "**üîπ Easy Meaning:**  \n",
    "Counts how many times each word appears in the sentence or document.  \n",
    "\n",
    "**üß† Example:**  \n",
    "**Sentences:**  \n",
    "1. \"I love NLP\"  \n",
    "2. \"I love Python and NLP\"  \n",
    "\n",
    "**Vocabulary** = [I, love, NLP, Python, and]  \n",
    "\n",
    "**Now count how many times each word appears:**  \n",
    "\n",
    "![Bag of Words Example](Bag%20of%20Words%20Example.png)  \n",
    "\n",
    "‚úÖ Good for basic text analysis and ML models.  \n",
    "\n",
    "\n",
    "**‚úÖ Advantages:**\n",
    "\n",
    "Simple to understand and implement ‚Äì great for beginners.\n",
    "\n",
    "Works well with traditional machine learning models.\n",
    "\n",
    "Fast to compute with small vocabulary sizes.\n",
    "\n",
    "Gives a solid baseline for text classification tasks.\n",
    "\n",
    "**‚ùå Disadvantages:**\n",
    "\n",
    "Ignores word order, so it loses meaning (e.g., \"not happy\" vs \"happy\").\n",
    "\n",
    "Vocabulary can become very large, especially with big datasets.\n",
    "\n",
    "Sparse vectors (many zeros) use a lot of memory.\n",
    "\n",
    "Does not capture context or relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    text  output\n",
      "0   people watch campusx       1\n",
      "1  campusx watch campusx       1\n",
      "2   people write comment       0\n",
      "3  campusx write comment       0\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with text data and output labels\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        'people watch campusx',\n",
    "        'campusx watch campusx',\n",
    "        'people write comment',\n",
    "        'campusx write comment'\n",
    "    ],\n",
    "    'output': [1, 1, 0, 0]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: ['campusx' 'comment' 'people' 'watch' 'write']\n",
      "Bag-of-Words matrix:\n",
      " [[1 0 1 1 0]\n",
      " [2 0 0 1 0]\n",
      " [0 1 1 0 1]\n",
      " [1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Create the Bag-of-Words representation\n",
    "bow = cv.fit_transform(df['text'])\n",
    "\n",
    "# To see the vocabulary and the actual matrix, you could add:\n",
    "print(\"\\nVocabulary:\", cv.get_feature_names_out())\n",
    "\n",
    "print(\"Bag-of-Words matrix:\\n\", bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **üì¶ 3. N-gram**  \n",
    "**üîπ Easy Meaning:**  \n",
    "It means combining N words together to understand context.  \n",
    "\n",
    "**üß† Examples:**  \n",
    "- **Unigram (1 word):** [\"I\", \"love\", \"NLP\"]  \n",
    "- **Bigram (2 words):** [\"I love\", \"love NLP\"]  \n",
    "- **Trigram (3 words):** [\"I love NLP\"]  \n",
    "\n",
    "‚úÖ Helps machine understand phrases, not just single words.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    text  output\n",
      "0   people watch campusx       1\n",
      "1  campusx watch campusx       1\n",
      "2   people write comment       0\n",
      "3  campusx write comment       0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with text data and output labels\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        'people watch campusx',\n",
    "        'campusx watch campusx',\n",
    "        'people write comment',\n",
    "        'campusx write comment'\n",
    "    ],\n",
    "    'output': [1, 1, 0, 0]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Create the Bag-of-Words representation\n",
    "bow = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch': 2, 'watch campusx': 4, 'campusx watch': 0, 'people write': 3, 'write comment': 5, 'campusx write': 1}\n",
      "Length =  6\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "print(cv.vocabulary_)\n",
    "print(\"Length = \",len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0]]\n",
      "[[1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ü§î Why move from N-gram to TF-IDF?**\n",
    "\n",
    "**üîπ First, understand what N-gram does:**\n",
    "\n",
    "It joins N words together to capture short phrases.\n",
    "\n",
    "**For example:**\n",
    "\n",
    "**Bigram:** \"I love\" or \"love NLP\"  \n",
    "**Trigram:** \"I love NLP\"\n",
    "\n",
    "‚úÖ It‚Äôs better than just counting single words because it starts to understand context (phrases).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå But N-gram has some problems:\n",
    "\n",
    "---\n",
    "\n",
    "| ‚ùå Problem               | üìñ What it means                                                                 |\n",
    "|-------------------------|----------------------------------------------------------------------------------|\n",
    "| **Too many combinations** | More words = more N-grams = bigger feature space (high memory usage)             |\n",
    "| **Still just counting**   | It doesn't know which words are important ‚Äî all words/phrases are treated equally |\n",
    "| **Common words dominate** | Common words like \"the\", \"and\", \"is\" appear a lot ‚Äî but don't add much meaning   |\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ So, we move to **TF-IDF** (Term Frequency ‚Äì Inverse Document Frequency)\n",
    "\n",
    "### üîπ Easy Meaning:\n",
    "\n",
    "> ‚ÄúIf a word appears a lot in **one document** but not much in others, it‚Äôs probably important.‚Äù\n",
    "\n",
    "TF-IDF gives **higher weight** to important words and **lowers weight** for common words.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è N-gram vs TF-IDF\n",
    "\n",
    "| Feature               | N-gram            | TF-IDF                         |\n",
    "|-----------------------|-------------------|--------------------------------|\n",
    "| Understand phrases?   | ‚úÖ Yes             | ‚úÖ Yes (when combined with N-gram) |\n",
    "| Weigh word importance?| ‚ùå No              | ‚úÖ Yes                          |\n",
    "| Handles common words? | ‚ùå No              | ‚úÖ Yes (reduces their weight)   |\n",
    "| Feature size          | üìà Very Large      | üìâ Smaller & more meaningful    |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† In Simple Words:\n",
    "\n",
    "- Use **N-gram** to capture phrases.\n",
    "- But use **TF-IDF** to make the model **focus on important words**, not just frequent ones.\n",
    "\n",
    "> üëâ Best of both worlds:  \n",
    "> Use `TfidfVectorizer(ngram_range=(1,2))` for **weighted n-grams**.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìö What is TF-IDF?**\n",
    "**TF-IDF stands for:**\n",
    "\n",
    "üëâ Term Frequency ‚Äì Inverse Document Frequency\n",
    "\n",
    "It‚Äôs a method to score words based on how important they are in a document compared to other documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Easy Meaning:**\n",
    "\n",
    "- **TF** = How often the word appears in one document  \n",
    "- **IDF** = How rare that word is across all documents\n",
    "\n",
    "---\n",
    "\n",
    "## **üß† Think like this:**\n",
    "\n",
    "- If a word appears many times in one document, it's probably important (TF).\n",
    "- But if that same word appears in every document, it's probably not special (IDF).\n",
    "\n",
    "‚úÖ So TF-IDF helps us find the **important and unique words** in each document.\n",
    "\n",
    "---\n",
    "\n",
    "## **üì¶ Example:**\n",
    "\n",
    "Imagine we have 3 sentences (documents):\n",
    "\n",
    "1. \"I love NLP\"  \n",
    "2. \"NLP is fun\"  \n",
    "3. \"I love Python and NLP\"\n",
    "\n",
    "**Now:**\n",
    "- The word **\"NLP\"** appears in all 3 documents ‚Üí common ‚Üí **lower IDF**\n",
    "- The word **\"Python\"** appears only once ‚Üí rare ‚Üí **higher IDF**\n",
    "\n",
    "üî∏ So, TF-IDF gives:\n",
    "- **Lower score** to common words (like \"NLP\")\n",
    "- **Higher score** to rare/unique words (like \"Python\")\n",
    "\n",
    "---\n",
    "\n",
    "# üì∏ TF-IDF Diagram 1 (Formula + Table View):\n",
    "\n",
    "![TF-IDF Diagram](TF-IDF%20Diagram.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why use TF-IDF?\n",
    "\n",
    "| ‚ùå Without TF-IDF (just count) | ‚úÖ With TF-IDF                    |\n",
    "|-------------------------------|----------------------------------|\n",
    "| All words treated equally     | Important words get higher scores |\n",
    "| Common words dominate         | Rare but meaningful words stand out |\n",
    "| No sense of word importance   | Smarter and more focused features  |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Simple Formula :\n",
    "\n",
    "**TF-IDF = TF √ó IDF**\n",
    "\n",
    "TF = (Word Count in Document)\n",
    "\n",
    "IDF = log(Total Documents / Documents containing the word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.49681612 0.         0.61366674 0.61366674 0.        ]\n",
      " [0.8508161  0.         0.         0.52546357 0.        ]\n",
      " [0.         0.57735027 0.57735027 0.         0.57735027]\n",
      " [0.49681612 0.61366674 0.         0.         0.61366674]]\n",
      "\n",
      "Feature Names (Vocabulary):\n",
      "['campusx' 'comment' 'people' 'watch' 'write']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data to TF-IDF features\n",
    "tfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Display feature names (vocabulary)\n",
    "print(\"\\nFeature Names (Vocabulary):\")\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **‚úçÔ∏è 4. Custom Features**  \n",
    "**üîπ Easy Meaning:**  \n",
    "You create your own features from the text ‚Äî based on your logic or goals.  \n",
    "\n",
    "**üß† Example:**  \n",
    "From this review: *\"I LOVED the movie!!! üòç\"*  \n",
    "You can create custom features like:  \n",
    "- Number of uppercase words ‚Üí **1**  \n",
    "- Has emojis ‚Üí **Yes**  \n",
    "- Contains ‚Äúloved‚Äù ‚Üí **Yes**  \n",
    "- Word count ‚Üí **5**  \n",
    "\n",
    "‚úÖ You choose what matters most. Great for ML models.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üß† 5. Word2Vec**  \n",
    "**üîπ Easy Meaning:**  \n",
    "Converts each word into a vector (set of numbers) that shows the meaning of the word, not just the word itself.  \n",
    "\n",
    "**üß† Example:**  \n",
    "- Word2Vec(\"king\") ‚Üí [0.12, 0.65, -0.33, ‚Ä¶]  \n",
    "- Word2Vec(\"queen\") ‚Üí [0.14, 0.68, -0.30, ‚Ä¶]  \n",
    "\n",
    "And guess what?  \n",
    "**king - man + woman ‚âà queen**  \n",
    "\n",
    "‚úÖ Word2Vec understands meaning and context (semantic similarity).  \n",
    "Much better than BoW or OHE for deep NLP work.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Summary of important NLP feature extraction techniques**  \n",
    "![Summary](<Summary important NLP feature extraction techniques.png>)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

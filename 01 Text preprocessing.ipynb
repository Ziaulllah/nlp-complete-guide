{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# 🧹 Text Preprocessing (Make Your Text Model-Ready!)\n",
    "\n",
    "## 🔍 What is Text Preprocessing?\n",
    "**🔹 Easy Meaning:**\n",
    "Text preprocessing means cleaning and organizing your raw text data so that machines can understand it.\n",
    "\n",
    "🧠 Think of it like washing vegetables before cooking. Clean text = Better results.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Why is It Important?\n",
    "**🔹 Because raw text is:**\n",
    "- Messy (symbols, emojis, punctuation)\n",
    "- Inconsistent (capital vs small letters)\n",
    "- Redundant (repeating words, stopwords)\n",
    "- Hard for machines to understand\n",
    "\n",
    "✨ Preprocessing helps your model focus on what really matters.\n",
    "\n",
    "---\n",
    "\n",
    "## 🪜 Common Text Preprocessing Steps\n",
    "\n",
    "### ⚙️ 1. Lowercasing\n",
    "**🔹 Meaning:** Convert all words to lowercase  \n",
    "**🧠 Example:** “Hello” → “hello”\n",
    "\n",
    "### ⚙️ 2. Remove Punctuation\n",
    "**🔹 Meaning:** Delete symbols (! ? . ,)  \n",
    "**🧠 Example:** “Wow!” → “wow”\n",
    "\n",
    "### ⚙️ 3. Tokenization\n",
    "**🔹 Meaning:** Break text into words  \n",
    "**🧠 Example:** “I love NLP” → [\"I\", \"love\", \"NLP\"]\n",
    "\n",
    "### ⚙️ 4. Remove Stopwords\n",
    "**🔹 Meaning:** Remove common words (the, is, at...)  \n",
    "**🧠 Example:** “I love the NLP” → [\"love\", \"NLP\"]\n",
    "\n",
    "### ⚙️ 5. Stemming\n",
    "**🔹 Meaning:** Cut words to their root form  \n",
    "**🧠 Example:** “Playing” → “play”\n",
    "\n",
    "### ⚙️ 6. Lemmatization\n",
    "**🔹 Meaning:** Convert to dictionary form  \n",
    "**🧠 Example:** “Better” → “good”\n",
    "\n",
    "### ⚙️ 7. Remove Numbers\n",
    "**🔹 Meaning:** Remove unnecessary digits  \n",
    "**🧠 Example:** “Python 3 is cool” → “Python is cool”\n",
    "\n",
    "### ⚙️ 8. Remove URLs/Emails\n",
    "**🔹 Meaning:** Clean links and emails  \n",
    "**🧠 Example:** “Visit xyz.com” → “Visit”\n",
    "\n",
    "### ⚙️ 9. Spell Correction\n",
    "**🔹 Meaning:** Fix typos  \n",
    "**🧠 Example:** “heello” → “hello”\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Python Tools for Preprocessing\n",
    "\n",
    "- `nltk` → Stopwords, tokenizing, stemming  \n",
    "- `spaCy` → Lemmatization, POS tagging  \n",
    "- `re` → Regex cleaning (remove links, numbers)  \n",
    "- `Ekphrasis` → For social media: emojis, hashtags, slangs  \n",
    "- `sklearn` → CountVectorizer, TfidfVectorizer includes preprocessing options  \n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Choose Steps Based on Your Task\n",
    "\n",
    "**🗂️ Example-Based Tips:**\n",
    "- Sentiment Analysis ➤ Keep \"not\", \"never\" (don’t remove all stopwords)\n",
    "- Spam Detection ➤ Keep numbers, links — they’re useful!\n",
    "- Topic Modeling ➤ Remove stopwords to discover main topics\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary:\n",
    "\"Text Preprocessing turns noisy, messy human language into clean, structured input that machines can learn from.\"\n",
    "\n",
    "🎉 Clean data = Smart Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower Case\n",
    "this is the First step in the Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production. <br /><br />the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. <br /><br />the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert one row to the lowercase\n",
    "df['review'][1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the Whole dataset or total rows and the specific column\n",
    "df['review']=df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove The HTML Tags\n",
    "now we will use the regular Expression in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production. <br /><br />the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. <br /><br />the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Re Library that will help on the Regular Expression\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub (r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we remove the URL from the review column\n",
    "df['review'] = df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the Punctuation Marks\n",
    "\"Remove punctuation marks\" means stripping out all non-alphanumeric symbols (like . , ! ? @ # $ %) from text data to clean and standardize it for analysis. This is a common step in Natural Language Processing (NLP) and text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With Punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "exclude = string.punctuation  # Note: this is string.punctuation (no underscore)\n",
    "\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = 'string. With. Punctuation!'\n",
    "\n",
    "cleaned_text = remove_punc(text)\n",
    "print(cleaned_text)  # Output: 'string With Punctuation'\n",
    "\n",
    "# start = time.time()\n",
    "# print(remove_punc(text))\n",
    "# time1 = time.time() - start\n",
    "# print(time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With Function from 00003000400050006  00a  Markovon\n",
      "0.0004169940948486328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import string\n",
    "\n",
    "text = \"string With Function from 0:0003:0004:0005:0006: + 00a + Markovon\"\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "#showing the total execution time\n",
    "start = time.time()\n",
    "print(remove_punc(text))\n",
    "time1 = time.time() - start\n",
    "print(time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat word Treatment \n",
    "\n",
    "In text preprocessing, \"Chat word Treatment\" refers to handling informal, colloquial, or slang words commonly used in chats, social media, and messaging platforms (e.g., \"u\" for \"you,\" \"brb\" for \"be right back,\" \"lol\" for \"laugh out loud\"). These words are non-standard and can affect NLP tasks if not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    'U': 'you',\n",
    "    'BRB': 'be right back',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'FYI': 'for your information',\n",
    "    'OMG': 'oh my god'\n",
    "    # Add more chat word mappings as needed\n",
    "}\n",
    "\n",
    "def char_conversion(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words:\n",
    "            new_text.append(chat_words[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 What is Ekphrasis?\n",
    "Ekphrasis is a Python library used to clean and understand messy, casual, or social media text.\n",
    "\n",
    " **Imagine people type like this:**\n",
    "\n",
    "idk what to do lol 😂\n",
    "\n",
    "OMG I'm sooo tired rn!!!\n",
    "\n",
    "visit http://example.com & check out!!!\n",
    "\n",
    "These kinds of text are hard for machines to understand.\n",
    "➡️ Ekphrasis helps convert them into proper, clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3669915737.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install ekphrasis\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install ekphrasis\n",
    "\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "\n",
    "# Initialize the text processor\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'time', 'date', 'number'],\n",
    "    unpack_contractions=True,        # expands \"can't\" to \"can not\"\n",
    "    spell_correct_elong=True,        # corrects elongated words like \"soooo\" → \"so\"\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\"},\n",
    "    fix_html=True,\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\",\n",
    "    tokenizer=\"social\"               # social tokenizer\n",
    ")\n",
    "\n",
    "# Define a new function (replacement for chat_conversion)\n",
    "def normalize_chat_text(text):\n",
    "    processed = text_processor.pre_process_doc(text)\n",
    "    return \" \".join(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "text = \"U should know this LOL\"\n",
    "converted_text = char_conversion(text)\n",
    "print(converted_text)  # Output: \"you should know this laugh out loud\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction\n",
    "Spelling correction is the process of identifying and fixing misspelled words in text data to improve consistency and accuracy for NLP tasks\n",
    "used for that profuse  like TextBlob  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob  \n",
    "\n",
    "incorrect_text = 'ceertain conditions during seveal generations are modified in the same maner.'\n",
    "\n",
    "# Initialize TextBlob object\n",
    "text_blob = TextBlob(incorrect_text)\n",
    "\n",
    "# Correct spelling and get the result\n",
    "corrected_text = text_blob.correct()\n",
    "\n",
    "print(\"Original text:\", incorrect_text)\n",
    "print(\"Corrected text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Word\n",
    "Stop words are common words (like \"the\", \"and\", \"is\") that are often filtered out from text data because they:\n",
    "\n",
    "Don't add meaningful information for many NLP tasks\n",
    "\n",
    "Increase noise without improving analysis\n",
    "\n",
    "Reduce processing efficiency (more words to process)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords data (only needed once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes English stopwords from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to process\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with stopwords removed\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word.lower() not in stop_words:  # Check lowercase version\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return ' '.join(new_text)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"this is an example sentence with some stop words\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Filtered:\", remove_stopwords(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#The re library is used for searching, matching,..\n",
    "#and manipulating text patterns using regular expressions (also called regex).\n",
    "#Think of it like a powerful search tool — but instead of looking for exact words, it looks for patterns in text.\n",
    "\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'  # dingbats\n",
    "        u'\\U000024C2-\\U0001F251'  # enclosed characters\n",
    "        ']+', flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Hello! 😊 This is a test 🚀 with emojis 🇺🇸 and symbols ✨\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {remove_emoji(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Emoji into the meanings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Hello! 😊'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Word tokenization\n",
    "\n",
    "**Word tokenization means:**\n",
    "\n",
    "Breaking a sentence into individual words.\n",
    "\n",
    "**Why is it Important in NLP?**\n",
    "Before a computer can understand text, it needs to break it \n",
    "down into parts (words).\n",
    "\n",
    "**This helps in:**\n",
    "\n",
    "Sentiment analysis\n",
    "\n",
    "Emotion detection\n",
    "\n",
    "Text classification\n",
    "\n",
    "Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "sent1 = \"I am going to Delhi\"\n",
    "word_tokens = sent1.split()\n",
    "print(\"Word tokens:\", word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sent2 = \"I am going to Delhi. I will stay there for 3 days. Let's hope the trip to be great.\"\n",
    "sentence_tokens = sent2.split('.')\n",
    "print(\"Sentence tokens:\", [s.strip() for s in sentence_tokens if s])  # Cleaned output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems with split function\n",
    "sent3 = \"I   am  going   to  Delhi\"  # Multiple spaces\n",
    "print(\"Simple split:\", sent3.split())  # Handles multiple spaces but not punctuation\n",
    "\n",
    "sent4 = \"I'm going to Delhi!\"\n",
    "print(\"Split with punctuation:\", sent4.split())  # Doesn't separate punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ NLTK (Natural Language Toolkit)\n",
    "**🟢 Easy Definition:**\n",
    "NLTK is like a schoolbook for learning language processing in Python.\n",
    "\n",
    "**📚 Use it when:**\n",
    "\n",
    "You want to learn how language works\n",
    "\n",
    "You want to try small text experiments\n",
    "\n",
    "You are a student or beginner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download required data (first time only)\n",
    "\n",
    "# Better word tokenization\n",
    "print(\"NLTK word tokens:\", word_tokenize(sent4))\n",
    "\n",
    "# Better sentence tokenization\n",
    "print(\"NLTK sentence tokens:\", sent_tokenize(sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ spaCy\n",
    "**🔵 Easy Definition:**\n",
    "spaCy is like a powerful tool made for real-world apps.\n",
    "\n",
    "**🚀 Use it when:**\n",
    "\n",
    "You want to build real projects (like chatbots, apps)\n",
    "\n",
    "You need speed and accuracy\n",
    "\n",
    "You are working on a job or company-level project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example sentences\n",
    "sent1 = \"I am going to Delhi\"\n",
    "sent5 = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "sent6 = \"This is a sample sentence for NLP processing\"\n",
    "sent7 = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Process each sentence with spaCy\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent5)\n",
    "doc3 = nlp(sent6)\n",
    "doc4 = nlp(sent7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is a text normalization technique in Natural Language Processing (NLP) that reduces words to their base or root form (called a \"stem\") by removing suffixes and prefixes.\n",
    "\n",
    "**Suffixes and Prefixes in Linguistics**\n",
    "\n",
    "Suffixes and prefixes are types of affixes (word parts added to a base word to modify its meaning or grammatical function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return ''.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'walk walks walking  walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization \n",
    "Lemmatization is a technique in Natural Language Processing (NLP) that reduces words to their base or dictionary form (called a lemma), ensuring the result is a valid word. Unlike stemming (which crudely chops off suffixes), lemmatization **uses:**\n",
    "\n",
    "Morphological analysis (grammar rules)\n",
    "\n",
    "Vocabulary/dictionary lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations = '?:!.,;'\n",
    "\n",
    "# Tokenize and remove punctuation\n",
    "sentence_words = word_tokenize(sentence)\n",
    "sentence_words = [word for word in sentence_words if word not in punctuations]\n",
    "\n",
    "# Print header\n",
    "print(\"{:<15} {:<15}\".format(\"Word\", \"Lemma\"))\n",
    "\n",
    "# Lemmatize each word and print results\n",
    "for word in sentence_words:\n",
    "    lemma = wordnet_lemmatizer.lemmatize(word, pos='v')  # Default is noun, 'v' for verbs\n",
    "    print(\"{:<15} {:<15}\".format(word, lemma))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

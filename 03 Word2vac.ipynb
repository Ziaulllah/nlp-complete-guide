{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ðŸ§  Word Embeddings**\n",
    "\n",
    "## ðŸ“Œ What are Word Embeddings?\n",
    "\n",
    "> **Word Embeddings** are a technique in Natural Language Processing (NLP) that **converts words into numerical vectors**, so that words with **similar meanings** have **similar vectors**.\n",
    "\n",
    "ðŸ§ Humans understand words by their meaning.  \n",
    "ðŸ¤– Computers only understand numbers.  \n",
    "âœ… So, **word embeddings help machines understand language** by turning words into **meaningful numbers**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§  Full Detailed Definition of Word Embeddings (Easy + In-Depth):\n",
    "Word Embeddings are a type of word representation that allows words with similar meaning to have similar numerical representations (vectors).\n",
    "\n",
    "In Natural Language Processing (NLP), word embeddings are used to transform words into dense vectors of real numbers, where the position of each word in the vector space is learned from a large amount of text. The idea is to capture the meaning of words, their syntactic roles, and their semantic relationships with other words.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Why are Word Embeddings special?\n",
    "\n",
    "Unlike Bag of Words or TF-IDF, word embeddings capture the **meaning**, **context**, and **relationship** between words.\n",
    "\n",
    "| ðŸ§± Traditional Methods   | ðŸš€ Word Embeddings                     |\n",
    "|--------------------------|----------------------------------------|\n",
    "| Just count word frequency| Capture **meaning and context**        |\n",
    "| Vectors are large/sparse | Vectors are small/dense/fixed-size     |\n",
    "| Ignores word meaning     | Understands **similar and related words** |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Example:\n",
    "\n",
    "Letâ€™s take four words:\n",
    "\n",
    "- **king**\n",
    "- **queen**\n",
    "- **man**\n",
    "- **woman**\n",
    "\n",
    "Word embeddings understand the relationship:\n",
    "\n",
    "**king - man + woman â‰ˆ queen**\n",
    "\n",
    "\n",
    "\n",
    "This works because word embeddings capture **gender and role** relationships in the vector space.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ How do they represent words?\n",
    "\n",
    "Each word is turned into a list of numbers (vector):\n",
    "\n",
    "| Word   | Vector (shortened)         |\n",
    "|--------|-----------------------------|\n",
    "| dog    | [0.73, 0.14, -0.59, ...]    |\n",
    "| puppy  | [0.75, 0.12, -0.55, ...]    |\n",
    "| table  | [-0.22, 0.91, 0.48, ...]    |\n",
    "\n",
    "âœ… Similar words (dog & puppy) have **similar vectors**  \n",
    "âŒ Unrelated words (dog & table) have **different vectors**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Common Types of Word Embeddings:\n",
    "\n",
    "| Method       | Description                                      |\n",
    "|--------------|--------------------------------------------------|\n",
    "| **Word2Vec** | Learns embeddings using neural networks          |\n",
    "| **GloVe**    | Combines word frequency & co-occurrence matrix   |\n",
    "| **FastText** | Understands even rare or unknown words (subwords)|\n",
    "| **BERT**     | Context-aware embeddings (meaning changes by context) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Why are Word Embeddings important?\n",
    "\n",
    "They are used to:\n",
    "\n",
    "- Understand word **similarity** and **relationships**\n",
    "- Improve performance of deep learning models (LSTM, RNN, BERT)\n",
    "- Power applications like:\n",
    "  - âœ… Chatbots  \n",
    "  - âœ… Search engines  \n",
    "  - âœ… Translation tools  \n",
    "  - âœ… Sentiment analysis  \n",
    "  - âœ… Text classification\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary in One Line\n",
    "\n",
    "> **Word Embeddings** turn words into meaningful numbers that reflect their **context and meaning**, helping machines process language like humans.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **ðŸ§  Word2Vec â€“ Easy and Practical Explanation**\n",
    "\n",
    "## ðŸ“Œ What is Word2Vec?\n",
    "\n",
    "> **Word2Vec** is a technique developed by Google that **learns word embeddings** â€” it turns words into **vectors of numbers** that capture their **meaning, context, and relationships** with other words.\n",
    "\n",
    "It helps the computer understand that:\n",
    "- \"king\" is related to \"queen\"\n",
    "- \"cat\" and \"dog\" are both animals\n",
    "- \"walk\" and \"walking\" are similar\n",
    "\n",
    "And it learns all of this automatically from raw text!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Why is it called \"Word2Vec\"?\n",
    "\n",
    "- Word2Vec = **Word to Vector**\n",
    "- It **converts a word â†’  into a vector**,  (a list of real numbers like [0.2, -0.4, 0.7, ...])\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ§  How does Word2Vec work?**\n",
    "\n",
    "Word2Vec trains a small **neural network** using a large text dataset in one of two ways:\n",
    "\n",
    "OR\n",
    "\n",
    "Word2Vec trains a neural network on a large text corpus using one of two methods:\n",
    "\n",
    "### 1. **CBOW (Continuous Bag of Words)**\n",
    "\n",
    "> Predicts a word from its context\n",
    "\n",
    "**Example:**  \n",
    "Input: â€œI ___ NLPâ€  \n",
    "Output: â€œloveâ€\n",
    "\n",
    "âœ… CBOW learns:  \n",
    "\"ðŸ‘‰ If the context is I and NLP, the likely word is 'love'\"\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Skip-Gram**\n",
    "\n",
    "> Predicts context from a word\n",
    "\n",
    "**Example:**  \n",
    "Input: â€œloveâ€  \n",
    "Output: likely neighbors â†’ â€œIâ€, â€œNLPâ€\n",
    "\n",
    "âœ… Skip-Gram learns:  \n",
    "ðŸ‘‰ \"If I see 'love', the surrounding words might be 'I' or 'NLP'.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸŽ¯ What does Word2Vec learn?**\n",
    "\n",
    "It learns vectors (embeddings) where:\n",
    "\n",
    "- Similar words have similar vector representations\n",
    "- It captures relationships like:\n",
    "\n",
    "**king - man + woman â‰ˆ queen**\n",
    "\n",
    "# These vectors are then used in:\n",
    "\n",
    "- Chatbots ðŸ¤–\n",
    "- Search engines ðŸ”\n",
    "- Translators ðŸŒ\n",
    "- Sentiment analysis ðŸ˜ŠðŸ˜¢\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Key Parameters\n",
    "\n",
    "| Parameter     | Description                                             |\n",
    "|---------------|---------------------------------------------------------|\n",
    "| `vector_size` | Number of dimensions in word vectors (e.g., 100)        |\n",
    "| `window`      | Context window size (how many words to the left/right)  |\n",
    "| `min_count`   | Ignore words that appear fewer times than this value    |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary (One Line)\n",
    "\n",
    "> **Word2Vec turns words into meaningful vectors using their context**, allowing machines to understand and compare words like humans do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import wget\n",
    "\n",
    "# Download the Google News vectors\n",
    "url = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "filename = wget.download(url)\n",
    "\n",
    "# Load the model (this will take some time and memory)\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# Now you can use the model\n",
    "# Example:\n",
    "print(model.most_similar('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After downloading and loading the model (as shown in previous code)\n",
    "# You can access word vectors like this:\n",
    "\n",
    "# Check the shape of a word vector (should be 300 dimensions)\n",
    "print(model['man'].shape)  # Output shown in image: (300,) \n",
    "\n",
    "# Note: The image shows (308,) which is unexpected - should be (300,)\n",
    "# This might indicate an issue with the model loading\n",
    "\n",
    "# Other examples of using the word vectors:\n",
    "# Get the vector for a word\n",
    "vector = model['woman']\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.most_similar('king', topn=5)\n",
    "print(similar_words)\n",
    "\n",
    "# Word analogy: king - man + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)  # Should output something like [('queen', 0.7...)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model['woman'].shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word similarity examples\n",
    "similarity_man_woman = model.similarity('man', 'woman')\n",
    "print(f\"Similarity between 'man' and 'woman': {similarity_man_woman:.4f}\")\n",
    "# Output shown: 0.7664\n",
    "\n",
    "similarity_man_php = model.similarity('man', 'PHP')\n",
    "print(f\"Similarity between 'man' and 'PHP': {similarity_man_php:.4f}\")\n",
    "# Output shown: -0.0330 (shows they're unrelated)\n",
    "\n",
    "# Odd one out example\n",
    "odd_one = model.doesnt_match(['PHP', 'java', 'monkey'])\n",
    "print(f\"The odd one out is: {odd_one}\")\n",
    "# Output shown: 'monkey' (as it's not a programming language)\n",
    "\n",
    "# Word analogy: king - man + woman = queen\n",
    "vector = model['king'] - model['man'] + model['woman']\n",
    "analogy_results = model.most_similar([vector], topn=3)\n",
    "\n",
    "print(\"\\nWord analogy results (king - man + woman):\")\n",
    "for word, score in analogy_results:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "# Expected to show 'queen' as top result with high similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currency analogy: INR is to India as GBP is to England\n",
    "vec = model['INR'] - model['India'] + model['England']\n",
    "currency_analogy_results = model.most_similar([vec], topn=5)\n",
    "\n",
    "print(\"Currency analogy results (INR - India + England):\")\n",
    "for word, score in currency_analogy_results:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "# Expected output similar to:\n",
    "# INR: 0.6442\n",
    "# GBP: 0.5841\n",
    "# England: 0.4465\n",
    "# ... (other related terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
